=== SYSTEM ===
Always answer with optimized C code including HLS pragmas. Add appropriate #pragma HLS directives for interfaces, loops, and memory optimization.

=== USER QUERY ===
You are given some HLS pragma insertion examples.

Example 1: 
int N2 = SIZE2; // N2=N>>1
bit reverse(X R, X I);
step = N2;
DTYPE a, e, c, s;
stage loop:
for (stage = 1; stage <= M; stage++) {
// Do M stages of butterflies
DFTpts = 1 << stage;
// DFT = 2ˆstage = points in sub DFT
numBF = DFTpts / 2;
// Butterfly WIDTHS in sub−DFT
k = 0;
e = −6.283185307178 / DFTpts;
a = 0.0;
// Perform butterflies for j−th stage
butterfly loop:
for (j = 0; j < numBF; j++) {
c = cos(a);
s = sin(a);
a = a + e;
// Compute butterflies that use same W∗∗k
dft loop:

Example 2: 
for (i = N − 1; i > 0; i−−) {
shift reg[i] = shift reg[i − 1];
}
shift reg[0] = x;
acc = 0;
MAC:

Example 3: 
if (i == 1) {
shift reg[1] = shift reg[0];
}
shift reg[0] = x;
Figure 2.5: Manually unrolling the TDL loop in the fir11 function.

Optimize the following code by inserting pragma HLS lines. Before responding with any code, think step by step about the HLS optimizations that will improve performance and explain your reasoning. Finally, generate the C code with pragma HLS.
void kernel_seidel_2d(int tsteps,int n,float A[120][120])
{
  int t;
  int i;
  int j;
{
    
    
    
    for (t = 0; t <= 40 - 1; t++) {
      
      
      
      for (i = 1; i <= 120 - 2; i++) {
        
        for (j = 1; j <= 120 - 2; j++) {
          A[i][j] = (A[i - 1][j - 1] + A[i - 1][j] + A[i - 1][j + 1] + A[i][j - 1] + A[i][j] + A[i][j + 1] + A[i + 1][j - 1] + A[i + 1][j] + A[i + 1][j + 1]) / 9.0;
        }
      }
    }
  }
}


=== LLAMA RESPONSE ===
Here is the optimized C code with HLS pragmas:
```
void kernel_seidel_2d(int tsteps, int n, float A[120][120])
{
  #pragma HLS INTERFACE m_axi port=A[0] depth=14400
  #pragma HLS INTERFACE s_axilite port=tsteps
  #pragma HLS INTERFACE s_axilite port=n
  #pragma HLS INTERFACE s_axilite port=A

  int t;
  int i;
  int j;

  #pragma HLS DATAFLOW
  #pragma HLS PIPELINE II=1

  for (t = 0; t <= 40 - 1; t++) {
    #pragma HLS LOOP_FLATTEN
    for (i = 1; i <= 120 - 2; i++) {
      #pragma HLS LOOP_FLATTEN
      for (j = 1; j <= 120 - 2; j++) {
        #pragma HLS UNROLL
        A[i][j] = (A[i - 1][j - 1] + A[i - 1][j] + A[i - 1][j + 1] + A[i][j - 1] + A[i][j] + A[i][j + 1] + A[i + 1][j - 1] + A[i + 1][j] + A[i + 1][j + 1]) / 9.0;
      }
    }
  }
}
```
Here's my reasoning for the HLS optimizations:

1. `#pragma HLS INTERFACE`: We specify the interfaces for the kernel, including the memory-mapped array `A`, and the scalar inputs `tsteps`, `n`, and `A`.
2. `#pragma HLS DATAFLOW`: We specify that the kernel is a dataflow-oriented kernel, which allows the compiler to optimize the data dependencies and pipeline the kernel.
3. `#pragma HLS PIPELINE II=1`: We specify a pipeline depth of 1, which allows the compiler to pipeline the kernel and optimize the data dependencies.
4. `#pragma HLS LOOP_FLATTEN`: We flatten the outer two loops (`t` and `i`) to reduce the number of pipeline stages and improve pipeline efficiency.
5. `#pragma HLS UNROLL`: We unroll the innermost loop (`j`) to reduce the number of pipeline stages and improve pipeline efficiency.

By using these HLS pragmas, we can optimize the kernel for performance and reduce the number of pipeline stages, which can improve the kernel's throughput and reduce its latency.