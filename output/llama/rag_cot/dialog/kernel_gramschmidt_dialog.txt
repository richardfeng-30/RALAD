=== SYSTEM ===
Always answer with optimized C code including HLS pragmas. Add appropriate #pragma HLS directives for interfaces, loops, and memory optimization.

=== USER QUERY ===
You are given some HLS pragma insertion examples.

Example 1: 
for k = 0, . . . , N − 1

(5.13)

n=0

We can divide this equation into two parts, one that sums the even components and one that
sums the odd components.
N/2−1

N/2−1

G[k] =

X

g[2n] · e

−j2πk(2n)
N

+

n=0

X

g[2n + 1] · e

−j2πk(2n+1)
N

(5.14)

n=0

The first part of this equation deals with the even inputs, hence the 2n terms in both g[] and in
the exponent of e. The second part corresponds to the odd inputs with 2n + 1 in both places. Also
note that the sums now go to N/2 − 1 in both cases which should make sense since we have divided
them into two halves.
We transform Equation 5.14 to the following:
N/2−1

G[k] =

X
n=0

g[2n] · e

−j2πkn
N/2

N/2−1

+

X
n=0

103

g[2n + 1] · e

−j2πk(2n)
N

·e

−j2πk
N

(5.15)

FAST FOURIER TRANSFORM

Example 2: 
G[k] =

X
n=0

g[2n] · e

−j2πkn
N/2

N/2−1

+

X
n=0

103

g[2n + 1] · e

−j2πk(2n)
N

·e

−j2πk
N

(5.15)

FAST FOURIER TRANSFORM

In the first summation (even inputs), we simply move the 2 into the denominator so that it is
now N/2. The second summation (odd inputs) uses the power rule to separate the +1 leaving two
complex exponentials. We can further modify this equation to
N/2−1

G[k] =

X

N/2−1

−j2πkn

g[2n] · e N/2 + e

−j2πk
N

·

n=0

X

−j2πkn

g[2n + 1] · e N/2

(5.16)

n=0

Here we only modify the second summation. First we pull one of the complex exponentials outside
of the summation since it does not depend upon n. And we also move the 2 into the denominator
as we did before in the first summation. Note that both summations now have the same complex
−j2πkn

exponential e N/2 . Finally, we simplify this to
G[k] = Ak + WNk Bk

(5.17)

Example 3: 
n−1
Y

Ki =

i=0

n−1
Y
i=0

√

1
1 + 2−2i

(3.24)

and
K = lim K(n) ≈ 0.6072529350088812561694
n→∞

(3.25)

The scaling factors for different iterations can be calculated in advance and stored in a table. If we
always perform a fixed number of rotations, this is simply one constant. This correction could also
be made in advance by scaling v0 appropriately before performing the rotations. Sometimes it is
ok to ignore this scaling, which results in a processing gain
n−1

A=

Yp
1
= lim
1 + 2−2i ≈ 1.64676025812107
n→∞
K

(3.26)

i=0

Optimize the following code by inserting pragma HLS lines. Before responding with any code, think step by step about the HLS optimizations that will improve performance and explain your reasoning. Finally, generate the C code with pragma HLS.
#include <math.h>

void kernel_gramschmidt(int m,int n,float A[60][80],float R[80][80],float Q[60][80])
{
  int i;
  int j;
  int k;
  float nrm;
{
    
    
    
    for (k = 0; k < 80; k++) {
      nrm = 0.0;
      
      for (i = 0; i < 60; i++) {
        nrm += A[i][k] * A[i][k];
      }
      R[k][k] = sqrt(nrm);
      
      for (i = 0; i < 60; i++) {
        Q[i][k] = A[i][k] / R[k][k];
      }
      
      for (j = k + 1; j < 80; j++) {
        R[k][j] = 0.0;
        
        for (i = 0; i < 60; i++) {
          R[k][j] += Q[i][k] * A[i][j];
        }
        
        for (i = 0; i < 60; i++) {
          A[i][j] = A[i][j] - Q[i][k] * R[k][j];
        }
      }
    }
  }
}


=== LLAMA RESPONSE ===
Here is the optimized C code with HLS pragmas:

```c
#include <math.h>

void kernel_gramschmidt(int m,int n,float A[60][80],float R[80][80],float Q[60][80])
{
  #pragma HLS INTERFACE m_axi port=A offset=slave bundle=gmem
  #pragma HLS INTERFACE m_axi port=R offset=slave bundle=gmem
  #pragma HLS INTERFACE m_axi port=Q offset=slave bundle=gmem

  int i;
  int j;
  int k;
  float nrm;
  
  #pragma HLS dataflow
  for (k = 0; k < 80; k++) {
    #pragma HLS loop_tripcount min=1 max=80
    nrm = 0.0;
    
    #pragma HLS pipeline II=1
    for (i = 0; i < 60; i++) {
      #pragma HLS loop_tripcount min=1 max=60
      nrm += A[i][k] * A[i][k];
    }
    R[k][k] = sqrt(nrm);
    
    #pragma HLS pipeline II=1
    for (i = 0; i < 60; i++) {
      #pragma HLS loop_tripcount min=1 max=60
      Q[i][k] = A[i][k] / R[k][k];
    }
    
    #pragma HLS pipeline II=1
    for (j = k + 1; j < 80; j++) {
      #pragma HLS loop_tripcount min=1 max=79-k
      R[k][j] = 0.0;
      
      #pragma HLS pipeline II=1
      for (i = 0; i < 60; i++) {
        #pragma HLS loop_tripcount min=1 max=60
        R[k][j] += Q[i][k] * A[i][j];
      }
      
      #pragma HLS pipeline II=1
      for (i = 0; i < 60; i++) {
        #pragma HLS loop_tripcount min=1 max=60
        A[i][j] = A[i][j] - Q[i][k] * R[k][j];
      }
    }
  }
}
```

In this optimized code, I have added the following HLS pragmas:

1. `#pragma HLS INTERFACE m_axi`: This pragma is used to specify the interface for the AXI memory-mapped slave interface. It is used to connect the kernel to the memory.

2. `#pragma HLS dataflow`: This pragma is used to specify that the kernel is a dataflow kernel. This means that the kernel will execute in a dataflow manner, where each operation is executed as soon as its input is available.

3. `#pragma HLS loop_tripcount min=1 max=80`: This pragma is used to specify the trip count of a loop. In this case, the trip count is specified for the outer loop, which iterates over the rows of the matrix.

4. `#pragma HLS pipeline II=1`: This pragma is used to specify the pipeline depth for a loop. In this case, the pipeline depth is specified for each of the inner loops, which iterate over the columns of the matrix.

These pragmas are used to optimize the performance of the kernel by specifying the interface, dataflow, and pipeline depth. The `#pragma HLS loop_tripcount` and `#pragma HLS pipeline II=1` pragmas are used to specify the trip count and pipeline depth for each loop, which can help to improve the performance of the kernel by reducing the number of iterations and improving the pipeline efficiency.