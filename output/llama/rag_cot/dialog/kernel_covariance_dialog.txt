=== SYSTEM ===
Always answer with optimized C code including HLS pragmas. Add appropriate #pragma HLS directives for interfaces, loops, and memory optimization.

=== USER QUERY ===
You are given some HLS pragma insertion examples.

Example 1: 
(4.9)

Note that the equations for deriving an , bn , and cn introduce the notion of a “negative” frequency. While this physically does not make much sense, mathematically we can think about as
a “negative” rotation on the complex plane. A “positive” frequency indicates that the complex
number rotates in a counterclockwise direction in the complex plane. A negative frequency simply
means that we are rotating in the opposite (clockwise) direction on the complex plane.
This idea is further illustrated by the relationship of cosine, sine, and the complex exponential.
Cosine can be viewed as the real part of the complex exponential and it can also be derived as the
sum of two complex exponentials – one with a positive frequency and the other with a negative
frequency as shown in Equation 4.10.
cos(x) = Re{ejx } =

ejx + e−jx
2

(4.10)

Example 2: 
n−1
Y

Ki =

i=0

n−1
Y
i=0

√

1
1 + 2−2i

(3.24)

and
K = lim K(n) ≈ 0.6072529350088812561694
n→∞

(3.25)

The scaling factors for different iterations can be calculated in advance and stored in a table. If we
always perform a fixed number of rotations, this is simply one constant. This correction could also
be made in advance by scaling v0 appropriately before performing the rotations. Sometimes it is
ok to ignore this scaling, which results in a processing gain
n−1

A=

Yp
1
= lim
1 + 2−2i ≈ 1.64676025812107
n→∞
K

(3.26)

i=0

Example 3: 
void spmv(int rowPtr[NUM ROWS+1], int columnIndex[NNZ],
DTYPE values[NNZ], DTYPE y[SIZE], DTYPE x[SIZE])
{
L1: for (int i = 0; i < NUM ROWS; i++) {
DTYPE y0 = 0;
L2: for (int k = rowPtr[i]; k < rowPtr[i+1]; k++) {
#pragma HLS unroll factor=8
#pragma HLS pipeline
y0 += values[k] ∗ x[columnIndex[k]];
}
y[i] = y0;
}
}
Figure 6.2: The baseline code for sparse matrix vector (SpMV) multiplication, which performs the
operation y = M · x. The variables rowPtr, columnIndex, and values hold M in CRS format. The
first for loop iterates across the rows while the second nested for loop iterates across the columns
of M by multiplying each non-zero element by the corresponding element in the vector x which
results in one element in the resulting vector y.

Optimize the following code by inserting pragma HLS lines. Before responding with any code, think step by step about the HLS optimizations that will improve performance and explain your reasoning. Finally, generate the C code with pragma HLS.

void kernel_covariance(int m,int n,float float_n,float data[100][80],float cov[80][80],float mean[80])
{
  int i;
  int j;
  int k;
{
    
    
    
    for (j = 0; j < 80; j++) {
      mean[j] = 0.0;
      
      for (i = 0; i < 100; i++) {
        mean[j] += data[i][j];
      }
      mean[j] /= float_n;
    }
    
    
    
    for (i = 0; i < 100; i++) {
      
      for (j = 0; j < 80; j++) {
        data[i][j] -= mean[j];
      }
    }
    
    
    
    for (i = 0; i < 80; i++) {
      
      for (j = i; j < 80; j++) {
        cov[i][j] = 0.0;
        
        for (k = 0; k < 100; k++) {
          cov[i][j] += data[k][i] * data[k][j];
        }
        cov[i][j] /= float_n - 1.0;
        cov[j][i] = cov[i][j];
      }
    }
  }
}


=== LLAMA RESPONSE ===
Here is the optimized C code with HLS pragmas:

```c
void kernel_covariance(int m, int n, float float_n, float data[100][80], float cov[80][80], float mean[80]) {
  #pragma HLS interface m_axi port=data[0] dim=2
  #pragma HLS interface m_axi port=cov[0] dim=2
  #pragma HLS interface m_axi port=mean[0] dim=1
  
  int i;
  int j;
  int k;
  
  // Calculate mean
  #pragma HLS dataflow
  for (j = 0; j < 80; j++) {
    #pragma HLS loop_tripcount 80
    mean[j] = 0.0;
    #pragma HLS pipeline
    for (i = 0; i < 100; i++) {
      #pragma HLS loop_tripcount 100
      mean[j] += data[i][j];
    }
    mean[j] /= float_n;
  }
  
  // Calculate variance
  #pragma HLS dataflow
  for (i = 0; i < 100; i++) {
    #pragma HLS loop_tripcount 100
    for (j = 0; j < 80; j++) {
      #pragma HLS loop_tripcount 80
      data[i][j] -= mean[j];
    }
  }
  
  // Calculate covariance
  #pragma HLS dataflow
  for (i = 0; i < 80; i++) {
    #pragma HLS loop_tripcount 80
    for (j = i; j < 80; j++) {
      #pragma HLS loop_tripcount 80
      cov[i][j] = 0.0;
      #pragma HLS pipeline
      for (k = 0; k < 100; k++) {
        #pragma HLS loop_tripcount 100
        cov[i][j] += data[k][i] * data[k][j];
      }
      cov[i][j] /= float_n - 1.0;
      cov[j][i] = cov[i][j];
    }
  }
}
```

Here's a step-by-step explanation of the HLS optimizations:

1.  The first step is to identify the loops and data dependencies in the code. In this case, the loops are nested, and there are data dependencies between the innermost loops.
2.  To optimize the code, we can use the `#pragma HLS loop_tripcount` directive to specify the number of iterations for each loop. This helps the compiler to generate more efficient code.
3.  We can also use the `#pragma HLS pipeline` directive to specify that the innermost loops should be pipelined. This allows the compiler to generate code that can execute multiple iterations of the loop in parallel.
4.  To optimize memory access, we can use the `#pragma HLS interface m_axi` directive to specify the memory interface for the arrays. This helps the compiler to generate code that can access the arrays in a more efficient way.
5.  Finally, we can use the `#pragma HLS dataflow` directive to specify that the code should be executed as a dataflow program. This allows the compiler to generate code that can execute the loops in parallel and optimize memory access.

By applying these HLS optimizations, we can significantly improve the performance of the code.