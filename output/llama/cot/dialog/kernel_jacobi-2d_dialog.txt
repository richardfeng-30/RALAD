=== SYSTEM ===
Always answer with optimized C code including HLS pragmas. Add appropriate #pragma HLS directives for interfaces, loops, and memory optimization.

=== USER QUERY ===
Optimize the following code by inserting pragma HLS lines. Before responding with any code, think step by step about the HLS optimizations that will improve performance and explain your reasoning. Finally, generate the C code with pragma HLS.

void kernel_jacobi_2d(int tsteps,int n,float A[90][90],float B[90][90])
{
  int t;
  int i;
  int j;
{
    
    
    
    for (t = 0; t < 40; t++) {
      
      
      
      for (i = 1; i < 90 - 1; i++) {
        
        for (j = 1; j < 90 - 1; j++) {
          B[i][j] = 0.2 * (A[i][j] + A[i][j - 1] + A[i][1 + j] + A[1 + i][j] + A[i - 1][j]);
        }
      }
      
      
      
      for (i = 1; i < 90 - 1; i++) {
        
        for (j = 1; j < 90 - 1; j++) {
          A[i][j] = 0.2 * (B[i][j] + B[i][j - 1] + B[i][1 + j] + B[1 + i][j] + B[i - 1][j]);
        }
      }
    }
  }
}


=== LLAMA RESPONSE ===
To optimize the code, we can consider the following HLS optimizations:

1.  Loop pipelining: By pipelining the loops, we can process multiple iterations of the loop in parallel, which can significantly improve the performance.

2.  Loop unrolling: Unrolling the loops can reduce the overhead of loop control statements and improve the cache locality.

3.  Memory optimization: By using the `#pragma HLS array_partition` directive, we can partition the arrays into smaller chunks that can be processed in parallel.

Here is the optimized C code with the HLS pragmas:

```c
void kernel_jacobi_2d(int tsteps,int n,float A[90][90],float B[90][90])
{
  #pragma HLS dataflow
  int t;
  int i;
  int j;
  #pragma HLS loop_tripcount min=40 max=40
  #pragma HLS loop_pipelineable
  
  for (t = 0; t < 40; t++) {
    #pragma HLS loop_unroll factor=9
    for (i = 1; i < 90 - 1; i++) {
      #pragma HLS loop_unroll factor=9
      for (j = 1; j < 90 - 1; j++) {
        B[i][j] = 0.2 * (A[i][j] + A[i][j - 1] + A[i][1 + j] + A[1 + i][j] + A[i - 1][j]);
      }
    }
    
    #pragma HLS loop_unroll factor=9
    for (i = 1; i < 90 - 1; i++) {
      #pragma HLS loop_unroll factor=9
      for (j = 1; j < 90 - 1; j++) {
        A[i][j] = 0.2 * (B[i][j] + B[i][j - 1] + B[i][1 + j] + B[1 + i][j] + B[i - 1][j]);
      }
    }
  }
  
  #pragma HLS array_partition variable=A cyclic factor=9
  #pragma HLS array_partition variable=B cyclic factor=9
}
```

In this code, we have added the following HLS pragmas:

*   `#pragma HLS dataflow`: This directive indicates that the function is a dataflow design, which means that the function can be executed in parallel.

*   `#pragma HLS loop_tripcount min=40 max=40`: This directive specifies the minimum and maximum number of iterations for the outer loop. This information can be used by the HLS compiler to optimize the loop.

*   `#pragma HLS loop_pipelineable`: This directive indicates that the loop can be pipelined, which means that multiple iterations of the loop can be processed in parallel.

*   `#pragma HLS loop_unroll factor=9`: This directive specifies the unrolling factor for the loops. The loops will be unrolled by the specified factor, which can reduce the overhead of loop control statements and improve the cache locality.

*   `#pragma HLS array_partition variable=A cyclic factor=9` and `#pragma HLS array_partition variable=B cyclic factor=9`: These directives partition the arrays `A` and `B` into smaller chunks that can be processed in parallel. The `cyclic` keyword indicates that the partitioning should be done in a cyclic manner, which means that the chunks will be processed in a circular order.

By using these HLS pragmas, we can optimize the code for performance and improve its execution speed.