=== SYSTEM ===
Always answer with optimized C code including HLS pragmas. Add appropriate #pragma HLS directives for interfaces, loops, and memory optimization.

=== USER QUERY ===
You are given some HLS pragma insertion examples.

Example 1: 
for (i = N − 1; i > 0; i−−) {
shift reg[i] = shift reg[i − 1];
}
shift reg[0] = x;
acc = 0;
MAC:

Example 2: 
void matrixmul(int A[N][M], int B[M][P], int AB[N][P]) {
#pragma HLS ARRAY RESHAPE variable=A complete dim=2
#pragma HLS ARRAY RESHAPE variable=B complete dim=1
/∗ for each row and column of AB ∗/
row: for(int i = 0; i < N; ++i) {
col: for(int j = 0; j < P; ++j) {
#pragma HLS PIPELINE II=1
/∗ compute (AB)i,j ∗/

Example 3: 
int i,j;
static dout_t acc;

}

LOOP_I:for(i=0; i < 20; i++){
LOOP_J: for(j=0; j < 20; j++){
acc += A[j] * i;
}
}
return acc;

In the above example, if the innermost (LOOP_J) is pipelined, there is one copy of LOOP_J in
hardware (a single multiplier). Vitis HLS automatically flattens the loops when possible, as in this
case, and effectively creates a new single loop (now called LOOP_I_LOOP_J) with 20*20
iterations. Only one multiplier operation and one array access need to be scheduled, then the
loop iterations can be scheduled as a single loop-body entity (20x20 loop iterations).
TIP: When a loop or function is pipelined, any loop in the hierarchy below the loop or function being
pipelined must be unrolled.

Optimize the following code by inserting pragma HLS lines.
void kernel_ludcmp(int n,float A[120][120],float b[120],float x[120],float y[120])
{
  int i;
  int j;
  int k;
  float w;
{
    
    
    
    for (i = 0; i < 120; i++) {
      
      for (j = 0; j < i; j++) {
        w = A[i][j];
        for (k = 0; k < j; k++) {
          w -= A[i][k] * A[k][j];
        }
        A[i][j] = w / A[j][j];
      }
      
      for (j = i; j < 120; j++) {
        w = A[i][j];
        for (k = 0; k < i; k++) {
          w -= A[i][k] * A[k][j];
        }
        A[i][j] = w;
      }
    }
    
    
    
    for (i = 0; i < 120; i++) {
      w = b[i];
      for (j = 0; j < i; j++) {
        w -= A[i][j] * y[j];
      }
      y[i] = w;
    }
    
    
    
    for (i = 0; i <= 119; i++) {
      int _in_i = 119 + -1 * i;
      w = y[_in_i];
      for (j = _in_i + 1; j < 120; j++) {
        w -= A[_in_i][j] * x[j];
      }
      x[_in_i] = w / A[_in_i][_in_i];
    }
    i = 0 + -1;
  }
}


=== LLAMA RESPONSE ===
Here is the optimized code with HLS pragmas:

```c
void kernel_ludcmp(int n, float A[120][120], float b[120], float x[120], float y[120]) {
  #pragma HLS INTERFACE m_axi port=A offset=slave bundle=gmem
  #pragma HLS INTERFACE m_axi port=b offset=slave bundle=gmem
  #pragma HLS INTERFACE m_axi port=x offset=slave bundle=gmem
  #pragma HLS INTERFACE m_axi port=y offset=slave bundle=gmem

  int i;
  int j;
  int k;
  float w;
  #pragma HLS DATAFLOW

  // Gaussian Elimination
  for (i = 0; i < 120; i++) {
    #pragma HLS LOOP_TRIPCOUNT min=120 max=120
    #pragma HLS PIPELINE II=1
    for (j = 0; j < i; j++) {
      w = A[i][j];
      for (k = 0; k < j; k++) {
        #pragma HLS UNROLL
        w -= A[i][k] * A[k][j];
      }
      A[i][j] = w / A[j][j];
    }
    for (j = i; j < 120; j++) {
      w = A[i][j];
      for (k = 0; k < i; k++) {
        #pragma HLS UNROLL
        w -= A[i][k] * A[k][j];
      }
      A[i][j] = w;
    }
  }

  // Forward Elimination
  for (i = 0; i < 120; i++) {
    #pragma HLS LOOP_TRIPCOUNT min=120 max=120
    #pragma HLS PIPELINE II=1
    w = b[i];
    for (j = 0; j < i; j++) {
      #pragma HLS UNROLL
      w -= A[i][j] * y[j];
    }
    y[i] = w;
  }

  // Backward Substitution
  for (i = 119; i >= 0; i--) {
    #pragma HLS LOOP_TRIPCOUNT min=120 max=120
    #pragma HLS PIPELINE II=1
    w = y[i];
    for (j = i + 1; j < 120; j++) {
      #pragma HLS UNROLL
      w -= A[i][j] * x[j];
    }
    x[i] = w / A[i][i];
  }
}
```

Note that I've added the necessary HLS pragmas to optimize the code for the Vitis HLS compiler. I've also rearranged the code to make it more suitable for HLS compilation. The Gaussian Elimination, Forward Elimination, and Backward Substitution steps are pipelined to improve parallelism. The loops are also unrolled to reduce the number of iterations and improve performance.