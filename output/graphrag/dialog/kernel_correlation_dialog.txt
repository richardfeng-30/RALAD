### Summary of Relevant Pragmas and Optimization Strategies for HLS

To optimize the given kernel for performance while controlling resource utilization and compilation time, several HLS pragma strategies can be applied:

1. **Loop Pipelining**: Applying `#pragma HLS PIPELINE` to loops can improve throughput by overlapping iterations, reducing initiation interval (II). This is especially useful for innermost loops with heavy computation.

2. **Loop Unrolling**: Using `#pragma HLS UNROLL factor=<N>` on loops with fixed bounds can increase parallelism by replicating hardware, but must be balanced to keep resource usage under control.

3. **Array Partitioning**: `#pragma HLS ARRAY_PARTITION` can be used to partition arrays to enable parallel access, reducing memory bottlenecks. For 2D arrays, partitioning along the dimension accessed in parallel is beneficial.

4. **Loop Tripcount**: Specifying tripcount pragmas helps the HLS tool optimize loops better and estimate performance.

5. **Dataflow Optimization**: Although not directly applicable here without restructuring, breaking the kernel into stages with dataflow can improve throughput.

6. **Resource Constraints**: To keep resource usage under 80%, unroll factors and partitioning should be moderate, and pipelining should be applied selectively.

7. **Pragma Placement**: Pragmas should be placed inside the function, directly before the loops they optimize.

### Applying Pragmas to the Provided Code

- The outer loops over `j` and `i` in mean and stddev calculations can be pipelined.
- The inner loops over `i` in mean and stddev can be unrolled partially or pipelined.
- The normalization loop over `i` and `j` can be pipelined and possibly unrolled on `j` since it has a fixed size of 80.
- The correlation computation involves nested loops; the innermost loop over `k` can be pipelined.
- Array partitioning on the second dimension of `data`, `mean`, `stddev`, and `corr` arrays can enable parallel access.

### Final Optimized Code with Pragmas

```c
#include <math.h>

void kernel_correlation(int m, int n, float float_n, float data[100][80], float corr[80][80], float mean[80], float stddev[80])
{
    float eps = 0.1;
    int i, j, k;

    // Partition arrays along the second dimension for parallel access
#pragma HLS ARRAY_PARTITION variable=data complete dim=2
#pragma HLS ARRAY_PARTITION variable=mean complete dim=1
#pragma HLS ARRAY_PARTITION variable=stddev complete dim=1
#pragma HLS ARRAY_PARTITION variable=corr complete dim=2

    // Compute mean
    for (j = 0; j < 80; j++) {
#pragma HLS PIPELINE II=1
        mean[j] = 0.0;
        for (i = 0; i < 100; i++) {
#pragma HLS PIPELINE II=1
            mean[j] += data[i][j];
        }
        mean[j] /= float_n;
    }

    // Compute stddev
    for (j = 0; j < 80; j++) {
#pragma HLS PIPELINE II=1
        stddev[j] = 0.0;
        for (i = 0; i < 100; i++) {
#pragma HLS PIPELINE II=1
            float diff = data[i][j] - mean[j];
            stddev[j] += diff * diff;
        }
        stddev[j] /= float_n;
        stddev[j] = sqrt(stddev[j]);
        stddev[j] = (stddev[j] <= eps ? 1.0 : stddev[j]);
    }

    // Normalize data
    for (i = 0; i < 100; i++) {
#pragma HLS PIPELINE II=1
        for (j = 0; j < 80; j++) {
#pragma HLS UNROLL factor=4
            data[i][j] -= mean[j];
            data[i][j] /= sqrt(float_n) * stddev[j];
        }
    }

    // Compute correlation matrix
    for (i = 0; i < 79; i++) {
#pragma HLS PIPELINE II=1
        corr[i][i] = 1.0;
        for (j = i + 1; j < 80; j++) {
#pragma HLS PIPELINE II=1
            corr[i][j] = 0.0;
            for (k = 0; k < 100; k++) {
#pragma HLS PIPELINE II=1
                corr[i][j] += data[k][i] * data[k][j];
            }
            corr[j][i] = corr[i][j];
        }
    }
    corr[79][79] = 1.0;
}
```

### Explanation

- **Array Partitioning**: Complete partitioning on the second dimension of `data` and `corr` arrays allows simultaneous access to all columns, enabling parallelism in loops over `j`.
- **Pipelining**: Applied to all major loops to improve throughput with initiation interval (II) of 1 where possible.
- **Unrolling**: The inner loop over `j` in normalization is unrolled by a factor of 4 to balance parallelism and resource usage.
- **Resource and Compilation Time**: The unroll factor and complete partitioning are chosen conservatively to keep resource usage under 80% and compilation time reasonable.

This approach leverages common HLS optimization pragmas to improve performance while controlling resource utilization and compilation time, as recommended in Vitis HLS optimization guidelines [Data: Entities (303, 357, 5775, 869, 8937, 8938, 8939)].